{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a27374d",
   "metadata": {},
   "source": [
    "RAG: Query translation --> Routing --> query construction -->indexing -->retrieval -->Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba06b2",
   "metadata": {},
   "source": [
    "Envirionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b4fff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = 'AIzaSyAN5VfS6Y73YD_KJ7RuT9O49B7GATfXRsk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99a91a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_classic import hub\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate #wraps the raw template string into a prompt object\n",
    "from langchain_core.output_parsers import StrOutputParser #extracts only the text and returns it\n",
    "\n",
    "from langchain_core.load import dumps, loads #dumps: convert a langchain object into a string, loads: convert string back into object\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.load import dumps, loads\n",
    "\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16750729",
   "metadata": {},
   "source": [
    "Part 1 -- the process of rag in general\n",
    "\n",
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47fea663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "#2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#3. Embed\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99500ee5",
   "metadata": {},
   "source": [
    "Generartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063cedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down complex or hard tasks into smaller, simpler, and more manageable steps or subgoals.\\n\\nIt can be done by:\\n*   LLMs with simple prompting (e.g., \"Steps for XYZ\", \"What are the subgoals for achieving XYZ?\").\\n*   Using task-specific instructions (e.g., \"Write a story outline\").\\n*   Human inputs.\\n\\nTechniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used for task decomposition. CoT involves instructing a model to \"think step by step\" to decompose tasks, while ToT extends this by exploring multiple reasoning possibilities and decomposing problems into multiple thought steps.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Prompt\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#5. LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "#6. Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#7. Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#8. Generation for the Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423cba3",
   "metadata": {},
   "source": [
    "Part 2 -- Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c3593",
   "metadata": {},
   "source": [
    "Option 1. Multi-Query: Improve retrieval quality by generating multiple variations of the user's question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a124ad5",
   "metadata": {},
   "source": [
    "step 1. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba136107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "#2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#3. Embed\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96ed56",
   "metadata": {},
   "source": [
    "Step 2. Prompt\n",
    "   Description: The process begins by using LLM to generate multiple reformulated queries derived from the original user question. These queries are then used to retrieve a diverse set of relevant documents, by unique union method. Finally, an LLM is applied once more to synthesize a comprehensive response, grounding the answer in the previously retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1986a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353533e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = generate_queries.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#return a list of unique document objects\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve retriever.map(): apply the retriever to each query in the list independently, which produce a list of list of documents\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc729c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For LLM agents, task decomposition involves the agent breaking down large tasks into smaller, manageable subgoals. This process enables the efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f41c01",
   "metadata": {},
   "source": [
    "Option 2 -- RAG-Fusion -- Combines multiple sets of retrieved documents produced by multiple queries, then rerank them to pick the best final context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1c7fd",
   "metadata": {},
   "source": [
    "Compared to option 1, after get mutiple queries, using reranking method, at here we apply reciprocal rank fusion to combine multiple ranked lists of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9918e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46089874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86356726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For LLM agents, **task decomposition** is the process where the agent breaks down large, complex tasks into smaller, more manageable subgoals. This enables the agent to handle complex tasks efficiently.\\n\\nTask decomposition can be achieved in several ways:\\n1.  **By the LLM itself** using simple prompts like \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\".\\n2.  **By using task-specific instructions**, such as \"Write a story outline\" for writing a novel.\\n3.  **With human inputs**.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16670088",
   "metadata": {},
   "source": [
    "Option 3 -- Decomposition -- Break down a complex, multiple-step problem into a set of smaller, more manageble sub-tasks. \n",
    "Papers: \n",
    "https://arxiv.org/pdf/2205.10625\n",
    "https://arxiv.org/pdf/2212.10509\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "157aba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: [q for q in x.split(\"\\n\") if q.strip() != \"\"]))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12e86dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to the main components of an LLM-powered autonomous agent system:',\n",
       " '1.  \"Main components of LLM autonomous agent architecture\"',\n",
       " '2.  \"Key modules in LLM-powered AI agents\"',\n",
       " '3.  \"Structure of autonomous agents using large language models\"']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1da152",
   "metadata": {},
   "source": [
    "Answer recursively, that means after get the answer of the first query, put the answer into the relevant documents before llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1f0a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccf97825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7009c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n---\\nQuestion: Here are three search queries related to the main components of an LLM-powered autonomous agent system:\\nAnswer: Here are three search queries related to the main components of an LLM-powered autonomous agent system:\\n\\n1.  \"Main components of LLM-powered autonomous agents\"\\n2.  \"Planning capabilities in LLM autonomous agents\"\\n3.  \"Memory systems for LLM autonomous agents\"\\n---\\nQuestion: 1.  \"Main components of LLM autonomous agent architecture\"\\nAnswer: In an LLM-powered autonomous agent system, the LLM functions as the agent\\'s core controller or \"brain.\" This is complemented by several key components:\\n\\n1.  **Planning:** This involves:\\n    *   **Subgoal and decomposition:** The agent breaks down large tasks into smaller, manageable subgoals to handle complex tasks efficiently.\\n    *   **Reflection and refinement:** The agent can perform self-criticism and self-reflection on past actions, learn from mistakes, and refine future steps to improve results.\\n2.  **Memory**\\n---\\nQuestion: 2.  \"Key modules in LLM-powered AI agents\"\\nAnswer: The key modules in LLM-powered AI agents include:\\n\\n1.  **LLM (Large Language Model):** Functions as the agent\\'s core controller or \"brain.\"\\n2.  **Planning:** This module involves:\\n    *   **Subgoal and decomposition:** Breaking down large tasks into smaller, manageable subgoals.\\n    *   **Reflection and refinement:** Performing self-criticism and self-reflection on past actions to learn from mistakes and refine future steps.\\n3.  **Memory:** A system for storing and retrieving information.\\n---\\nQuestion: 3.  \"Structure of autonomous agents using large language models\"\\nAnswer: The structure of autonomous agents using large language models (LLMs) typically consists of the following key components:\\n\\n1.  **LLM (Large Language Model):** This functions as the agent\\'s core controller or \"brain,\" driving its reasoning and decision-making.\\n2.  **Planning:** This module enables the agent to strategize and execute tasks, and it includes:\\n    *   **Subgoal and decomposition:** The ability to break down complex tasks into smaller, manageable subgoals.\\n    *   **Reflection and refinement:** The capacity for self-criticism and self-reflection on past actions, allowing the agent to learn from mistakes and refine its future steps.\\n3.  **Memory:** A system for storing and retrieving information, essential for retaining knowledge and context over time.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ef9b6",
   "metadata": {},
   "source": [
    "Answer individually -- the answer for each question combined and input into another llm to get the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32987364",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag  = hub.pull(\"rlm/rag-prompt\")\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9412b9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSH (Locality-Sensitive Hashing) uses a hashing function to map similar input items to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs. ANNOY (Approximate Nearest Neighbors Oh Yeah) employs random projection trees, where non-leaf nodes split the input space and leaves store data points. ANNOY search aggregates results from all trees by iteratively finding the closest half to the query, mimicking a hashing function and offering greater scalability than KD trees.',\n",
       " \"The main components of LLM autonomous agents include the LLM itself, which functions as the agent's brain. This core is complemented by key components such as Planning. Planning involves breaking down large tasks into subgoals, and using reflection and refinement to learn from past actions.\",\n",
       " \"In an LLM-powered AI agent system, the Large Language Model (LLM) acts as the agent's brain. This core is complemented by key components such as Planning and Memory. Planning involves breaking down tasks into subgoals and using reflection to refine actions and learn from mistakes.\",\n",
       " \"In LLM-powered autonomous agent systems, the LLM serves as the agent's brain. This core is complemented by key modules such as Planning and Memory. Planning further includes subgoal decomposition and reflection for refinement of actions.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b46a92a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to the input question:',\n",
       " '1.  \"Main components of LLM autonomous agents\"',\n",
       " '2.  \"Architecture of LLM-powered AI agents\"',\n",
       " '3.  \"Key modules in autonomous LLM systems\"']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fe60b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main components of an LLM-powered autonomous agent system include:\\n\\n1.  **The Large Language Model (LLM):** This serves as the agent's brain, driving its core intelligence and decision-making.\\n2.  **Planning:** This crucial component involves breaking down large tasks into smaller, manageable subgoals. It also incorporates mechanisms for reflection and refinement, enabling the agent to learn from past actions and mistakes, and to adapt its strategies.\\n3.  **Memory:** This module complements the LLM and Planning, providing the agent with the ability to store and retrieve information, which is essential for maintaining context and facilitating long-term learning.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96981a",
   "metadata": {},
   "source": [
    "Option 4 -- Step back -- based on the paper https://arxiv.org/pdf/2310.06117, it is a prompting technique to create a high-level\n",
    "abstraction of the problem before answering it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76d7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f76b6bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do LLM agents approach complex problems?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ba9fe",
   "metadata": {},
   "source": [
    "Note: Use itemgetter when you only need to fetch a key or index. Use lambda if you need any computation, transformation, or conditional logic on the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "060a5f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For LLM agents, task decomposition is a key component of their planning capabilities. It involves the agent breaking down large, complex tasks into smaller, more manageable subgoals. This process is essential for enabling the efficient handling of complex tasks by allowing the agent to approach them in a structured, step-by-step manner.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ab64f",
   "metadata": {},
   "source": [
    "Option 5 -- HyDE: Having the LLM generate a fake but plausible answer first, and then using that generated text as the query for similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bb9c770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Task Decomposition for LLM Agents\\n\\nTask decomposition, in the context of Large Language Model (LLM) agents, refers to the strategic process of breaking down a complex, high-level objective into a series of smaller, more manageable, and interdependent sub-tasks or sub-goals. This approach mirrors human cognitive strategies for problem-solving, where intricate problems are often tackled by segmenting them into simpler, sequential, or hierarchical steps.\\n\\nThe primary motivation for employing task decomposition in LLM agents stems from the inherent limitations of monolithic approaches, where an LLM is expected to solve an entire complex problem in a single, unguided inference step. Such monolithic execution often leads to increased cognitive load, higher error rates, reduced interpretability, and difficulty in handling long-horizon dependencies or intricate logical reasoning. Decomposition mitigates these issues by:\\n\\n1.  **Reducing Complexity:** Each sub-task presents a more constrained and focused problem, allowing the LLM to leverage its reasoning capabilities more effectively without being overwhelmed by the overall task\\'s breadth.\\n2.  **Improving Accuracy and Robustness:** By addressing smaller components, the LLM can generate more precise and relevant outputs for each step. Errors can be localized to specific sub-tasks, making debugging and self-correction more feasible.\\n3.  **Enhancing Interpretability and Explainability:** The step-by-step execution provides a clear trace of the agent\\'s thought process and actions, making it easier for human users to understand how a solution was derived and identify potential points of failure.\\n4.  **Facilitating Planning and Reasoning:** Decomposition naturally lends itself to structured planning. The LLM agent, often guided by an internal \"planner\" or \"reasoning module,\" analyzes the initial prompt, identifies key components, and generates a structured plan outlining the sequence of sub-tasks. Intermediate outputs from one sub-task frequently serve as inputs for subsequent steps, creating a coherent problem-solving trajectory.\\n5.  **Enabling Self-Correction and Adaptation:** With a decomposed task, the agent can evaluate the outcome of each sub-task. If an output is unsatisfactory or leads to an impasse, the agent can backtrack, re-plan, or re-execute a specific sub-task without having to restart the entire process.\\n\\nVarious methodologies facilitate task decomposition in LLM agents, including explicit planning frameworks, Chain-of-Thought (CoT) prompting, Tree-of-Thought (ToT) prompting, and frameworks like ReAct (Reasoning and Acting) which interleave reasoning steps with external tool use. The effectiveness of task decomposition heavily relies on the agent\\'s ability to accurately identify relevant sub-tasks, manage their dependencies, and synthesize their individual solutions into a coherent final output that addresses the original complex objective.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# HyDE document generation\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "# llm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm| StrOutputParser() \n",
    ")\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96b26605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retrieved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce22ad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, task decomposition for LLM agents is a technique used to break down complex or hard tasks into smaller, simpler, and more manageable steps.\\n\\nIt is achieved by:\\n*   Instructing the model to \"think step by step\" (e.g., Chain of Thought).\\n*   Transforming big tasks into multiple manageable tasks.\\n*   Exploring multiple reasoning possibilities at each step, generating multiple thoughts per step, and creating a tree structure (e.g., Tree of Thoughts).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retrieved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aabc2a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
