{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a27374d",
   "metadata": {},
   "source": [
    "RAG: Query translation --> Routing --> query construction -->indexing -->retrieval -->Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba06b2",
   "metadata": {},
   "source": [
    "Envirionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b4fff58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = 'AIzaSyAN5VfS6Y73YD_KJ7RuT9O49B7GATfXRsk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a91a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ZhangHui\\.conda\\envs\\env_cuda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_classic import hub\n",
    "from langchain_text_splitters  import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16750729",
   "metadata": {},
   "source": [
    "Part 1 -- the process of rag in general\n",
    "\n",
    "Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fea663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "#2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#3. Embed\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99500ee5",
   "metadata": {},
   "source": [
    "Generartion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1063cedc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is the process of breaking down complex or hard tasks into smaller, simpler, and more manageable steps or subgoals.\\n\\nIt can be done by:\\n*   LLMs with simple prompting (e.g., \"Steps for XYZ\", \"What are the subgoals for achieving XYZ?\").\\n*   Using task-specific instructions (e.g., \"Write a story outline\").\\n*   Human inputs.\\n\\nTechniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used for task decomposition. CoT involves instructing a model to \"think step by step\" to decompose tasks, while ToT extends this by exploring multiple reasoning possibilities and decomposing problems into multiple thought steps.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Prompt\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "#5. LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "#6. Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "#7. Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "#8. Generation for the Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2423cba3",
   "metadata": {},
   "source": [
    "Part 2 -- Query Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526c3593",
   "metadata": {},
   "source": [
    "Option 1. Multi-Query: Improve retrieval quality by generating multiple variations of the user's question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a124ad5",
   "metadata": {},
   "source": [
    "step 1. Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba136107",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "#2. Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "#3. Embed\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding_model)\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96ed56",
   "metadata": {},
   "source": [
    "Step 2. Prompt\n",
    "   Description: The process begins by using LLM to generate multiple reformulated queries derived from the original user question. These queries are then used to retrieve a diverse set of relevant documents, by unique union method. Finally, an LLM is applied once more to synthesize a comprehensive response, grounding the answer in the previously retrieved context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1986a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate #wraps the raw template string into a prompt object\n",
    "from langchain_core.output_parsers import StrOutputParser #extracts only the text and returns it\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6353533e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = generate_queries.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e60e5d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads #dumps: convert a langchain object into a string, loads: convert string back into object\n",
    "#return a list of unique document objects\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve retriever.map(): apply the retriever to each query in the list independently, which produce a list of list of documents\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddc729c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For LLM agents, task decomposition involves the agent breaking down large tasks into smaller, manageable subgoals. This process enables the efficient handling of complex tasks.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f41c01",
   "metadata": {},
   "source": [
    "Option 2 -- RAG-Fusion -- Combines multiple sets of retrieved documents produced by multiple queries, then rerank them to pick the best final context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1c7fd",
   "metadata": {},
   "source": [
    "Compared to option 1, after get mutiple queries, using reranking method, at here we apply reciprocal rank fusion to combine multiple ranked lists of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1f9918e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# RAG-Fusion: Related\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7897ba06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion \n",
    "    | ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46089874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\" Reciprocal_rank_fusion that takes multiple lists of ranked documents \n",
    "        and an optional parameter k used in the RRF formula \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to hold fused scores for each unique document\n",
    "    fused_scores = {}\n",
    "\n",
    "    # Iterate through each list of ranked documents\n",
    "    for docs in results:\n",
    "        # Iterate through each document in the list, with its rank (position in the list)\n",
    "        for rank, doc in enumerate(docs):\n",
    "            # Convert the document to a string format to use as a key (assumes documents can be serialized to JSON)\n",
    "            doc_str = dumps(doc)\n",
    "            # If the document is not yet in the fused_scores dictionary, add it with an initial score of 0\n",
    "            if doc_str not in fused_scores:\n",
    "                fused_scores[doc_str] = 0\n",
    "            # Retrieve the current score of the document, if any\n",
    "            previous_score = fused_scores[doc_str]\n",
    "            # Update the score of the document using the RRF formula: 1 / (rank + k)\n",
    "            fused_scores[doc_str] += 1 / (rank + k)\n",
    "\n",
    "    # Sort the documents based on their fused scores in descending order to get the final reranked results\n",
    "    reranked_results = [\n",
    "        (loads(doc), score)\n",
    "        for doc, score in sorted(fused_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    ]\n",
    "\n",
    "    # Return the reranked results as a list of tuples, each containing the document and its fused score\n",
    "    return reranked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() | reciprocal_rank_fusion\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86356726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For LLM agents, **task decomposition** is the process where the agent breaks down large, complex tasks into smaller, more manageable subgoals. This enables the agent to handle complex tasks efficiently.\\n\\nTask decomposition can be achieved in several ways:\\n1.  **By the LLM itself** using simple prompts like \"Steps for XYZ.\" or \"What are the subgoals for achieving XYZ?\".\\n2.  **By using task-specific instructions**, such as \"Write a story outline\" for writing a novel.\\n3.  **With human inputs**.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16670088",
   "metadata": {},
   "source": [
    "Option 3 -- Decomposition -- Break down a complex, multiple-step problem into a set of smaller, more manageble sub-tasks. \n",
    "Papers: \n",
    "https://arxiv.org/pdf/2205.10625\n",
    "https://arxiv.org/pdf/2212.10509\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "157aba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: [q for q in x.split(\"\\n\") if q.strip() != \"\"]))\n",
    "\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12e86dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to the main components of an LLM-powered autonomous agent system:',\n",
       " '1.  \"Main components of LLM autonomous agent architecture\"',\n",
       " '2.  \"Key modules in LLM-powered AI agents\"',\n",
       " '3.  \"Structure of autonomous agents using large language models\"']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1da152",
   "metadata": {},
   "source": [
    "Answer recursively, that means after get the answer of the first query, put the answer into the relevant documents before llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d1f0a7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccf97825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "# llm\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7009c315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n---\\nQuestion: Here are three search queries related to the main components of an LLM-powered autonomous agent system:\\nAnswer: Here are three search queries related to the main components of an LLM-powered autonomous agent system:\\n\\n1.  \"Main components of LLM-powered autonomous agents\"\\n2.  \"Planning capabilities in LLM autonomous agents\"\\n3.  \"Memory systems for LLM autonomous agents\"\\n---\\nQuestion: 1.  \"Main components of LLM autonomous agent architecture\"\\nAnswer: In an LLM-powered autonomous agent system, the LLM functions as the agent\\'s core controller or \"brain.\" This is complemented by several key components:\\n\\n1.  **Planning:** This involves:\\n    *   **Subgoal and decomposition:** The agent breaks down large tasks into smaller, manageable subgoals to handle complex tasks efficiently.\\n    *   **Reflection and refinement:** The agent can perform self-criticism and self-reflection on past actions, learn from mistakes, and refine future steps to improve results.\\n2.  **Memory**\\n---\\nQuestion: 2.  \"Key modules in LLM-powered AI agents\"\\nAnswer: The key modules in LLM-powered AI agents include:\\n\\n1.  **LLM (Large Language Model):** Functions as the agent\\'s core controller or \"brain.\"\\n2.  **Planning:** This module involves:\\n    *   **Subgoal and decomposition:** Breaking down large tasks into smaller, manageable subgoals.\\n    *   **Reflection and refinement:** Performing self-criticism and self-reflection on past actions to learn from mistakes and refine future steps.\\n3.  **Memory:** A system for storing and retrieving information.\\n---\\nQuestion: 3.  \"Structure of autonomous agents using large language models\"\\nAnswer: The structure of autonomous agents using large language models (LLMs) typically consists of the following key components:\\n\\n1.  **LLM (Large Language Model):** This functions as the agent\\'s core controller or \"brain,\" driving its reasoning and decision-making.\\n2.  **Planning:** This module enables the agent to strategize and execute tasks, and it includes:\\n    *   **Subgoal and decomposition:** The ability to break down complex tasks into smaller, manageable subgoals.\\n    *   **Reflection and refinement:** The capacity for self-criticism and self-reflection on past actions, allowing the agent to learn from mistakes and refine its future steps.\\n3.  **Memory:** A system for storing and retrieving information, essential for retaining knowledge and context over time.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ef9b6",
   "metadata": {},
   "source": [
    "Answer individually -- the answer for each question combined and input into another llm to get the final answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32987364",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag  = hub.pull(\"rlm/rag-prompt\")\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9412b9bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LSH (Locality-Sensitive Hashing) uses a hashing function to map similar input items to the same buckets with high probability, where the number of buckets is much smaller than the number of inputs. ANNOY (Approximate Nearest Neighbors Oh Yeah) employs random projection trees, where non-leaf nodes split the input space and leaves store data points. ANNOY search aggregates results from all trees by iteratively finding the closest half to the query, mimicking a hashing function and offering greater scalability than KD trees.',\n",
       " \"The main components of LLM autonomous agents include the LLM itself, which functions as the agent's brain. This core is complemented by key components such as Planning. Planning involves breaking down large tasks into subgoals, and using reflection and refinement to learn from past actions.\",\n",
       " \"In an LLM-powered AI agent system, the Large Language Model (LLM) acts as the agent's brain. This core is complemented by key components such as Planning and Memory. Planning involves breaking down tasks into subgoals and using reflection to refine actions and learn from mistakes.\",\n",
       " \"In LLM-powered autonomous agent systems, the LLM serves as the agent's brain. This core is complemented by key modules such as Planning and Memory. Planning further includes subgoal decomposition and reflection for refinement of actions.\"]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b46a92a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to the input question:',\n",
       " '1.  \"Main components of LLM autonomous agents\"',\n",
       " '2.  \"Architecture of LLM-powered AI agents\"',\n",
       " '3.  \"Key modules in autonomous LLM systems\"']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3fe60b3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The main components of an LLM-powered autonomous agent system include:\\n\\n1.  **The Large Language Model (LLM):** This serves as the agent's brain, driving its core intelligence and decision-making.\\n2.  **Planning:** This crucial component involves breaking down large tasks into smaller, manageable subgoals. It also incorporates mechanisms for reflection and refinement, enabling the agent to learn from past actions and mistakes, and to adapt its strategies.\\n3.  **Memory:** This module complements the LLM and Planning, providing the agent with the ability to store and retrieve information, which is essential for maintaining context and facilitating long-term learning.\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee96981a",
   "metadata": {},
   "source": [
    "Option 4 -- Step back -- based on the paper https://arxiv.org/pdf/2310.06117, it is a prompting technique to create a high-level\n",
    "abstraction of the problem before answering it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c76d7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few Shot Examples\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f76b6bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How do LLM agents approach complex problems?'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2ff713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6ba9fe",
   "metadata": {},
   "source": [
    "Note: Use itemgetter when you only need to fetch a key or index. Use lambda if you need any computation, transformation, or conditional logic on the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "060a5f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For LLM agents, task decomposition is a key component of their planning capabilities. It involves the agent breaking down large, complex tasks into smaller, more manageable subgoals. This process is essential for enabling the efficient handling of complex tasks by allowing the agent to approach them in a structured, step-by-step manner.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982ab64f",
   "metadata": {},
   "source": [
    "Option 5 -- HyDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb9c770",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
